"""
Script de vectorisation des articles MySQL â†’ ChromaDB
Lance : python scripts/run_vectorization.py
"""
import sys
from pathlib import Path
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Ajouter le dossier racine au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import text
from config.database import get_engine
from src.embeddings import split_text_into_chunks, estimate_cost, OpenAIEmbeddings
from src.vectorstore import ChromaManager


def load_articles_from_db():
    """Charge tous les articles depuis MySQL."""
    engine = get_engine()
    query = text("""
                 SELECT id, source, title, url, date_published, content, language
                 FROM articles
                 WHERE content IS NOT NULL AND content != ''
                 ORDER BY id
                 """)

    with engine.connect() as conn:
        result = conn.execute(query)
        articles = [dict(row._mapping) for row in result]

    return articles


def main():
    """Point d'entrÃ©e principal."""
    print("\n" + "=" * 60)
    print("ğŸš€ VECTORISATION DES ARTICLES")
    print("=" * 60)

    # 1. Charger les articles
    print("\nğŸ“‚ Chargement des articles depuis MySQL...")
    articles = load_articles_from_db()
    print(f"   âœ… {len(articles)} articles chargÃ©s")

    if not articles:
        print("   âš ï¸  Aucun article Ã  vectoriser")
        return

    # 2. Chunking
    print("\nâœ‚ï¸  DÃ©coupage des articles en chunks...")
    all_chunks = []
    total_tokens = 0

    for article in articles:
        metadata = {
            "article_id": article["id"],
            "source": article["source"],
            "title": article["title"],
            "url": article["url"],
            "date": str(article["date_published"]) if article["date_published"] else None,
            "language": article["language"]
        }

        chunks = split_text_into_chunks(article["content"], metadata)
        all_chunks.extend(chunks)
        total_tokens += sum(chunk["token_count"] for chunk in chunks)

    print(f"   âœ… {len(all_chunks)} chunks crÃ©Ã©s")
    print(f"   ğŸ“Š Total tokens : {total_tokens:,}")
    print(f"   ğŸ’° CoÃ»t estimÃ© : ${estimate_cost(total_tokens):.4f}")

    # 3. Confirmation
    response = input("\nğŸ‘‰ Continuer avec la vectorisation ? (y/n): ").lower().strip()
    if response != 'y':
        print("âŒ Vectorisation annulÃ©e")
        return

    # 4. GÃ©nÃ©ration des embeddings
    print("\nğŸ¤– GÃ©nÃ©ration des embeddings via OpenAI...")
    embeddings_client = OpenAIEmbeddings()

    texts = [chunk["text"] for chunk in all_chunks]
    embeddings = embeddings_client.embed_batch(texts)

    # 5. Stockage dans ChromaDB
    print("\nğŸ’¾ Stockage dans ChromaDB...")
    chroma = ChromaManager()

    # RÃ©initialiser la collection si demandÃ©
    if chroma.count_documents() > 0:
        print(f"   âš ï¸  La collection contient dÃ©jÃ  {chroma.count_documents()} documents")
        response = input("   ğŸ‘‰ RÃ©initialiser ? (y/n): ").lower().strip()
        if response == 'y':
            chroma.reset_collection()

    # PrÃ©parer les donnÃ©es (filtrer les None des mÃ©tadonnÃ©es)
    ids = [f"chunk_{i}" for i in range(len(all_chunks))]
    metadatas = []

    for chunk in all_chunks:
        # Filtrer les valeurs None
        clean_metadata = {
            k: v for k, v in chunk["metadata"].items()
            if v is not None
        }
        metadatas.append(clean_metadata)

    # DEBUG
    print(f"\nğŸ” DEBUG:")
    print(f"   len(texts) = {len(texts)}")
    print(f"   len(embeddings) = {len(embeddings)}")
    print(f"   len(metadatas) = {len(metadatas)}")
    print(f"   len(ids) = {len(ids)}")
    print(f"   Exemple metadata: {metadatas[0] if metadatas else 'VIDE'}")

    # Ajouter par batch pour Ã©viter les timeouts
    batch_size = 100
    for i in range(0, len(texts), batch_size):
        end_idx = min(i + batch_size, len(texts))
        print(f"   Ajout batch {i // batch_size + 1}/{(len(texts) - 1) // batch_size + 1}...")

        chroma.add_documents(
            texts=texts[i:end_idx],
            embeddings=embeddings[i:end_idx],
            metadatas=metadatas[i:end_idx],
            ids=ids[i:end_idx]
        )

    # 6. Statistiques finales
    print("\n" + "=" * 60)
    print("âœ… VECTORISATION TERMINÃ‰E")
    print("=" * 60)

    stats = embeddings_client.get_usage_stats()
    print(f"\nğŸ“Š Statistiques :")
    print(f"   â€¢ Articles traitÃ©s : {len(articles)}")
    print(f"   â€¢ Chunks crÃ©Ã©s : {len(all_chunks)}")
    print(f"   â€¢ Tokens utilisÃ©s : {stats['total_tokens']:,}")
    print(f"   â€¢ CoÃ»t total : ${stats['estimated_cost_usd']:.4f}")
    print(f"   â€¢ Documents dans ChromaDB : {chroma.count_documents()}")

    print("\nğŸ¯ Prochaine Ã©tape : python scripts/search_rag.py")
    print("=" * 60)


if __name__ == "__main__":
    main()