"""
Interface web Streamlit pour le RAG Amundi avec g√©n√©ration LLM
Lance : streamlit run app.py
"""
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# IMPORTANT : Charger le .env AVANT tout le reste
load_dotenv()

from src.embeddings import OpenAIEmbeddings
from src.vectorstore import ChromaManager
from sqlalchemy import text
from config.database import get_engine

# Configuration de la page
st.set_page_config(
    page_title="Amundi RAG | Veille R√©glementaire",
    page_icon="üîç",
    layout="wide"
)

# CSS personnalis√©
st.markdown("""
    <style>
    .main {
        padding: 2rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #0066cc;
        color: white;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .result-card {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #0066cc;
        margin-bottom: 1rem;
    }
    .answer-box {
        background-color: #e8f5e9;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #4caf50;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)


@st.cache_resource
def get_chroma():
    """Cache ChromaDB manager."""
    return ChromaManager()


@st.cache_resource
def get_embeddings_client():
    """Cache OpenAI embeddings client."""
    return OpenAIEmbeddings()


@st.cache_resource
def get_openai_client():
    """Cache OpenAI client pour GPT."""
    return OpenAI()


def get_db_stats():
    """R√©cup√®re les stats de la base."""
    engine = get_engine()
    with engine.connect() as conn:
        # Total articles
        total = conn.execute(text("SELECT COUNT(*) FROM articles")).fetchone()[0]

        # Par source
        result = conn.execute(text("""
            SELECT source, COUNT(*) as count 
            FROM articles 
            GROUP BY source 
            ORDER BY count DESC
        """))
        by_source = {row[0]: row[1] for row in result}

    return total, by_source


def search_rag(query, n_results=5, source_filter=None):
    """Effectue une recherche RAG."""
    embeddings_client = get_embeddings_client()
    chroma = get_chroma()

    # G√©n√©rer embedding de la question
    query_embedding = embeddings_client.embed_text(query)

    # Rechercher
    where_filter = {"source": source_filter} if source_filter else None
    results = chroma.search(
        query_embedding=query_embedding,
        n_results=n_results,
        where=where_filter
    )

    return results


def generate_answer(query, results):
    """G√©n√®re une r√©ponse avec GPT bas√©e sur les r√©sultats RAG."""

    if not results["ids"] or not results["ids"][0]:
        # Aucun r√©sultat trouv√©
        return {
            "answer": "‚ùå Je n'ai trouv√© aucun document pertinent dans ma base de donn√©es pour r√©pondre √† cette question. Essayez de reformuler votre question ou de v√©rifier l'orthographe.",
            "sources_used": 0,
            "model": None
        }

    # Construire le contexte √† partir des r√©sultats
    context_parts = []
    sources_info = []

    for i, (text, metadata, distance) in enumerate(zip(
        results["documents"][0][:3],  # Top 3 r√©sultats
        results["metadatas"][0][:3],
        results["distances"][0][:3]
    ), 1):
        score = (1 - distance) * 100

        if score > 30:  # Seuil de pertinence
            context_parts.append(f"""
[Document {i}]
Source: {metadata.get('source')}
Titre: {metadata.get('title')}
Date: {metadata.get('date', 'N/A')}
Contenu: {text[:1000]}
""")
            sources_info.append({
                'source': metadata.get('source'),
                'title': metadata.get('title'),
                'url': metadata.get('url'),
                'score': score
            })

    if not context_parts:
        return {
            "answer": "‚ö†Ô∏è J'ai trouv√© des documents mais ils ne semblent pas assez pertinents pour r√©pondre √† votre question avec confiance. Pouvez-vous reformuler votre question de mani√®re plus pr√©cise ?",
            "sources_used": 0,
            "model": None
        }

    context = "\n\n".join(context_parts)

    # Prompt pour GPT
    system_prompt = """Tu es un assistant expert en r√©glementation financi√®re europ√©enne.
Tu r√©ponds aux questions en te basant UNIQUEMENT sur les documents fournis.

R√®gles importantes :
- R√©ponds en fran√ßais de mani√®re claire et structur√©e
- Cite pr√©cis√©ment les sources (num√©ro du document)
- Si l'information n'est pas dans les documents, dis-le clairement
- Sois pr√©cis et factuel
- Utilise des bullet points si n√©cessaire
"""

    user_prompt = f"""Question : {query}

Documents de r√©f√©rence :
{context}

R√©ponds √† la question en te basant sur ces documents. Cite les sources [Document X] dans ta r√©ponse."""

    # Appel √† GPT
    client = get_openai_client()

    response = client.chat.completions.create(
        model="gpt-5-nano",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        max_completion_tokens=1000
    )

    answer = response.choices[0].message.content

    return {
        "answer": answer,
        "sources_used": len(sources_info),
        "sources_info": sources_info,
        "model": "gpt-5-nano"
    }


# Header
st.title("üîç Amundi GEM | Assistant R√©glementaire")
st.markdown("üí¨ *Posez vos questions, je r√©ponds en me basant sur les documents officiels*")
st.markdown("---")

# Sidebar - Statistiques
with st.sidebar:
    st.header("üìä Base de donn√©es")

    total_articles, by_source = get_db_stats()
    chroma_docs = get_chroma().count_documents()

    col1, col2 = st.columns(2)
    with col1:
        st.metric("Articles", total_articles)
    with col2:
        st.metric("Chunks", chroma_docs)

    st.markdown("---")
    st.subheader("üìÇ Par source")
    for source, count in by_source.items():
        st.text(f"{source}: {count}")

    st.markdown("---")
    st.info("ü§ñ **Mod√®le :** gpt-5-nano")
    st.caption("üí° Propuls√© par OpenAI & ChromaDB")

# Main content
query = st.text_area(
    "Votre question",
    placeholder="Ex: Quelle est la position de l'AFG sur la facturation √©lectronique ?",
    help="Posez une question en fran√ßais ou en anglais",
    height=100
)

# Param√®tres
col1, col2, col3 = st.columns([2, 1, 1])

with col1:
    sources = ["Toutes"] + list(by_source.keys())
    source_filter = st.selectbox("Filtrer par source", sources)
    if source_filter == "Toutes":
        source_filter = None

with col2:
    n_results = st.selectbox("Documents", [3, 5, 10], index=1)

with col3:
    show_sources = st.checkbox("Afficher sources", value=True)

# Bouton de recherche
if st.button("ü§ñ Obtenir une r√©ponse", use_container_width=True, type="primary"):
    if not query:
        st.warning("‚ö†Ô∏è Veuillez entrer une question")
    else:
        with st.spinner("üîç Recherche des documents pertinents..."):
            results = search_rag(query, n_results, source_filter)

        with st.spinner("üí≠ G√©n√©ration de la r√©ponse..."):
            answer_data = generate_answer(query, results)

        # Afficher la r√©ponse
        with st.container():
            st.markdown("### üí° R√©ponse")
            st.markdown(answer_data['answer'])

        # M√©tadonn√©es
        if answer_data['model']:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Sources utilis√©es", answer_data['sources_used'])
            with col2:
                st.metric("Mod√®le", answer_data['model'])

        # Afficher les sources si demand√©
        if show_sources and answer_data['sources_used'] > 0:
            st.markdown("---")
            st.subheader("üìö Sources consult√©es")

            for i, source_info in enumerate(answer_data['sources_info'], 1):
                with st.expander(f"üìÑ Source {i} - {source_info['source']} (Score: {source_info['score']:.1f}%)"):
                    st.markdown(f"**Titre :** {source_info['title']}")
                    st.markdown(f"**URL :** [{source_info['url']}]({source_info['url']})")

        # Documents d√©taill√©s
        if show_sources and results["ids"] and results["ids"][0]:
            with st.expander("üîé Voir tous les documents trouv√©s"):
                for i, (doc_id, text, metadata, distance) in enumerate(zip(
                    results["ids"][0],
                    results["documents"][0],
                    results["metadatas"][0],
                    results["distances"][0]
                ), 1):

                    score = (1 - distance) * 100

                    st.markdown(f"**Document {i}** - Score: {score:.1f}%")
                    st.text(f"Source: {metadata.get('source')} | Date: {metadata.get('date', 'N/A')}")
                    st.text(f"Titre: {metadata.get('title')[:80]}...")
                    st.markdown(f"[Lien]({metadata.get('url')})")
                    st.markdown("---")

# Instructions
if not query:
    st.info("üí° **Comment utiliser cet assistant ?**")
    st.markdown("""
    1. **Posez votre question** dans le champ ci-dessus
    2. **Filtrez par source** si vous cherchez dans une autorit√© sp√©cifique (optionnel)
    3. **Cliquez sur "Obtenir une r√©ponse"** pour lancer la recherche
    4. **L'assistant g√©n√®re une r√©ponse** bas√©e sur les documents officiels
    
    **Exemples de questions :**
    - "Quelle est la position de l'AFG sur la facturation √©lectronique ?"
    - "Quelles sont les obligations de reporting EMIR 3.0 ?"
    - "R√©sume les principales mesures concernant les fonds d'investissement"
    - "Qu'est-ce que la grille d'impact pour la dette priv√©e ?"
    """)

# Footer
st.markdown("---")
st.caption("üè¶ Amundi Asset Management | Assistant IA de Veille R√©glementaire")